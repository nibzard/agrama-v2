# Matryoshka Bloom-Filtered Embedding Cascade (MBFEC) System Specification

## System Overview and Development Phases

The **Matryoshka Bloom-Filtered Embedding Cascade (MBFEC)** system is a novel memory indexing and retrieval architecture designed for AI agents. It will be developed in two major phases:

* **Phase 1 – Embedded Single-Machine Prototype:** An open-source implementation running on a single machine (no external dependencies beyond the host). This prototype focuses on core functionality and ultra-low latency retrieval for a single agent’s memory. It emphasizes CPU-based computation and efficient in-memory data structures.

* **Phase 2 – Cloud-Based Multi-Agent Platform:** An evolved version suitable for deployment in a distributed cloud environment. In this phase, multiple AI agents share and contribute to a *collective memory graph*, enabling cross-agent retrieval and knowledge sharing. The platform will support concurrency, synchronization, and eventual consistency across agents’ memory, building on the robust core developed in Phase 1.

The MBFEC system’s overarching goal is to provide an AI agent (or a society of agents) with a **temporal memory** that can be queried in real time. It leverages *hierarchical (matryoshka) embeddings* and a *Bloom-filter cascade* to achieve sub-millisecond retrieval of recent context, even as the memory grows. In later phases it extends to multi-agent scenarios where shared memory and *latent link prediction* allow agents to learn from each other’s experiences.

## Use Case: Temporal Agent Workflow and Memory Graph

The primary use case is an LLM-driven autonomous agent executing a task through a sequence of steps. Each step in the workflow – whether it’s a user instruction, the agent’s thought, a tool API call, or an observation result – is captured as a **node in a temporal memory graph**. Key characteristics of this memory graph use case:

* **Temporal Sequence:** Nodes are naturally connected in chronological order (each node links to the next). This forms a *timeline of context* for the agent’s ongoing task. The memory is segmented into *time windows* to reflect recency (more on this in **Temporal Indexing** below).

* **Vectorized Memory Nodes:** Each memory node carries a vector embedding representation of its content (e.g. the text of the step, or a summary of the action). These embeddings enable semantic recall: the agent can later retrieve a past step not by exact keyword, but by similarity in meaning (e.g. recalling a previous step where the user “expressed frustration about service quality” by searching semantically).

* **Agent Actions and Observations:** The memory captures both the agent’s internal decisions and external tool interactions. For example, if the agent performed a web search and read a result, the content summary of that result is stored as an embedding node. This allows retrieval of *experiences* (like “finding a relevant document”) and *outcomes* (like “the answer found in that document”) when similar situations arise.

* **Graph Relationships:** Besides the inherent time sequence edges, the memory graph can form additional links. Future versions of MBFEC will infer *latent links* between non-consecutive nodes – for instance, linking two past situations that dealt with a similar concept or problem, even if they occurred far apart in time. These links might be established through clustering or learned predictors that identify related contexts. Over time, this enriches the graph so that relevant memory can be retrieved not just by direct recency, but also via these semantic connections.

* **User and Agent Access:** Functionally, the system will allow the agent to **store** each new step into memory and **retrieve** relevant prior information on demand. For example, if the agent encounters a task it has seen before, it can query the memory with the current context embedding and retrieve the most similar past episodes to inform its next action. This happens automatically as part of the agent’s reasoning loop – effectively giving the agent a form of long-term memory beyond the immediate context window.

In summary, MBFEC serves as the agent’s **working and long-term memory**, storing a rolling history of interactions and enabling the agent to *remember* and *reuse* knowledge from recent and past events. This memory is temporal (old memories eventually expire or get compressed) and semantic (retrieval is meaning-based via embeddings).

## Technical Architecture and Key Components

### Memory Node Representation and Hierarchical Embeddings

Each memory node consists of the raw data of the step (e.g. text or other modality), metadata (timestamp, step type, etc.), and a set of **hierarchical embeddings**. These embeddings follow the *Matryoshka* principle: a single high-dimensional vector embedding contains multiple nested representations at different granularities. In practical terms, the embedding model is trained to “front-load” information in the vector such that the first few components capture the most salient features, and additional components add finer details. This means:

* The **full embedding** (e.g. 768 dimensions for a text embedding) encodes the complete semantic content of the node.

* Truncated prefixes of this embedding (e.g. first 128 dims, 256 dims, etc.) give coarser but still meaningful representations of the same content. This is analogous to Russian nesting dolls: smaller “dolls” (prefixes) fit inside larger ones and provide an approximate representation of the whole.

* Crucially, these *Matryoshka embeddings* allow a trade-off between accuracy and speed. Using a smaller embedding yields faster comparisons at the cost of some precision, whereas the full embedding gives maximum accuracy. The system can thus **cascade** from coarse to fine similarity checking using these nested embeddings.

Each node therefore stores something like: `{ id, timestamp, content, embedding_full, embedding_half, embedding_quarter, ... }` (the exact granularity levels can be configured, e.g. 1/4, 1/8 of full dimensions, etc.). This hierarchical representation is central to MBFEC’s ability to quickly shortlist candidates and then refine results, as described next.

### Bloom-Filtered Embedding Cascade (Retrieval Pipeline)

At the heart of MBFEC is the **embedding cascade retrieval** mechanism. This is a multi-stage pipeline that uses *Bloom filter-based screening* at each stage to drastically reduce search space before moving to the next, more detailed stage. The stages correspond to the hierarchical embedding sizes:

1. **Coarse Embedding Stage:** When a new query arrives (e.g. the agent’s current context embedding), MBFEC first takes a *small prefix* of the query’s embedding – for example, the first 1/32 of dimensions (a very coarse vector). This low-dimensional vector is used to probe a **Bloom filter index** that quickly tells whether any stored node is *potentially* similar at this coarse granularity.

   * **Distance-Sensitive Bloom Filter:** Traditional Bloom filters answer set membership exactly, but here we need to answer “are there any vectors in memory within a coarse similarity radius of this query?” We achieve this by using a learned hash or locality-sensitive hashing scheme that maps the coarse embedding to a set of hash bits. Each memory node’s coarse embedding is also hashed and inserted into the Bloom filter. If the query’s hash bits **do not appear** in the filter, we can conclude no memory node is close enough at this coarse level (avoiding needless work). If the filter returns a *possible match*, we move to the next stage. This design ensures we never falsely miss a relevant candidate (no false negatives, barring hash collisions), though we might get false positives that we’ll discard later. Using Bloom filters for such multi-dimensional, approximate matching is inspired by prior systems like multi-level Bloom indices and BitFunnel, adapted here for embedding similarity.

   * **Learned Bloom Filters:** To improve efficiency, MBFEC can employ *learned Bloom filters* – where a small ML model assists in deciding filter membership in tandem with the bit array. Recent research shows cascaded learned BFs can significantly reduce memory and computation for membership queries. By balancing model size and filter size, a cascaded learned BF can minimize the *reject time* (time to confidently say “no match”) and cut memory use by \~24%. In MBFEC, a learned BF variant can be trained on the distribution of coarse embedding hashes, yielding faster rejection of irrelevant queries and more space-efficient filters. The cascade of model + filter acts like a sieve at each granularity, quickly filtering out most memory nodes that are clearly not similar to the query.

2. **Intermediate Embedding Stages:** If the coarse stage passes (i.e., the Bloom filter indicates some candidates might exist in recent memory), MBFEC then considers a larger portion of the embedding (say 1/16 or 1/8 of dimensions). The system maintains **multiple Bloom filter layers**, each corresponding to a finer-grained embedding representation. At each layer, the query’s embedding (truncated to that layer’s size) is hashed and checked against the filter for that layer. Each successive filter has a tighter criterion (because more dimensions = more specific semantic detail), thereby further narrowing the candidate set. This *multi-granularity cascade* of Bloom filters progressively homes in on the most relevant memories, while discarding false positives from earlier, coarser checks.

   * The cascade is analogous to a **funnel search**: start broad and cheap, then incrementally refine. For instance, an initial search on 1/32-dim embeddings yields a broad pool of candidates; then re-evaluating those with 1/16-dim (or checking a finer BF) prunes many candidates; then 1/8-dim, etc., until we reach near-full dimensions. Importantly, the expensive full-dimensional comparison is deferred until the very end, after we’ve drastically cut down the candidate list.

   * Each Bloom filter stage is *distance-sensitive* in the sense that the hashing scheme or learned model is tuned for that embedding scale’s notion of similarity. In practice, this could involve using locality-sensitive hashing functions or quantization that preserve neighborhood structure for that dimensionality. For example, a coarse embedding might be rounded or bucketed, and those bucket IDs inserted into the filter; only if a query falls into an occupied bucket do we proceed. Learned models could also predict the probability that a given memory node is similar to the query and decide to allow or filter it out probabilistically, as part of the cascade logic.

3. **Fine Retrieval Stage (HNSW Index):** After passing through all Bloom filter layers, we end up with a small set of candidate memory nodes that are likely relevant. For these candidates, MBFEC performs a final similarity search using a high-precision vector index. We use a **Hierarchical Navigable Small World (HNSW)** graph for this final stage to get the top-\$k\$ nearest neighbors by the full embedding. HNSW is a state-of-the-art approximate nearest neighbor structure that organizes vectors in multiple layers of a proximity graph. It provides very fast \$k\$-NN search (logarithmic average complexity) by navigating from coarse graph layers to fine. In MBFEC, we build a **compressed HNSW** index on the memory nodes:

   * *Compressed/Pruned Graph:* Since the Bloom filters have drastically reduced the search set, the HNSW index can be much smaller or limited to recent items. We can also apply vector compression like Product Quantization (PQ) on the stored vectors to reduce memory and speed up distance calculations. The compressed representation is used to find approximate distances, and only for the final ranking do we use the true vector values if needed. This two-tier approach (compressed graph + exact re-ranking) keeps latency low.

   * In essence, HNSW acts as the *re-ranking* mechanism: given the few dozen (or hundred) candidates that made it through the cascade, HNSW quickly finds which ones are truly closest to the query in the full embedding space, and returns the top results. Because HNSW itself is hierarchical, it aligns well with our approach – it has long links in upper layers for speed and short links in lower layers for accuracy. We ensure the HNSW index is kept **small** by only indexing a window of recent memory (and perhaps any particularly important older nodes), to maintain sub-millisecond query time.

**Real-Time Performance:** The above cascade is tuned for **real-time retrieval**, prioritizing the latest context. By using very cheap operations in the early stages (bit checks in Bloom filters, small vector dot products), the system can respond to queries extremely fast – the target is *sub-1 millisecond* latency for retrieving recent context matches. This focuses on *precision in the now* rather than exhaustive recall over all history. If no recent memory is relevant, the system can optionally fall back to older memory (with a higher latency budget) or simply conclude no strong match exists. The design choice here is to favor immediacy: the agent’s most recent experiences (e.g. within the last few minutes or hours) are made **instantly accessible**, trading off thorough scanning of deep history. In practice, this greatly benefits an agent’s responsiveness, as recent events are often the most salient for the next decisions.

It’s worth noting that the Bloom filter cascade may occasionally produce *false positives* – meaning it lets through some nodes that aren’t truly similar – but these are efficiently handled by the HNSW re-ranker and by the nature of Bloom filters **never producing false negatives**. In other words, if something is truly relevant, it is extremely likely to make it through the cascade; the cost is only that we might carry a bit of chaff along with the wheat, which the final ranking will sort out. The cascade thus ensures high recall of recent relevant memories, while keeping the overall search time and computations minimal by preemptively filtering out the bulk of irrelevant data.

### Temporal Indexing and Ring-Buffer Memory Organization

MBFEC organizes memory in **temporal segments** to reflect the principle that *recent memories are more important (and accessed faster) than distant ones*. Instead of a monolithic vector store, we use a rolling **time-window index**:

* **Time Windows:** Memory nodes are partitioned into fixed-duration or fixed-count windows. For example, the system might partition memory by hour-long blocks, or by every 1000 nodes. Windows are arranged in chronological order. Only a configurable number of most recent windows are kept “active” in memory (e.g. the last N hours or last N thousand steps, depending on resource limits).

* **Ring Buffer Implementation:** These windows are managed in a ring buffer fashion. When a new window begins (i.e. as time progresses and the oldest window expires), the oldest window’s slot is reused for the newest data. This implies a **garbage collection by expiry**: once a window falls off the retention horizon, its data is scheduled for deletion or archiving. All associated data structures (Bloom filters, indices for that window) are cleared or recycled. This keeps the memory footprint bounded and ensures that query latency remains stable over time, because we never accumulate unbounded history in the active index. Recent windows (those in the ring buffer) form the agent’s *working set* of memory.

* **Per-Window Indices:** Each time window maintains its own Bloom filter cascade and local HNSW (or similar) index. The retrieval process will typically start by querying the **latest window** (most recent) and then proceed backwards only if needed. This means the newest window – which likely contains the immediate conversational context – can be searched extremely fast (the index is small, and likely fits in CPU cache). Only if that yields insufficient results do we check the previous window, and so on. The effect is that *99% of queries might be satisfied by looking at just the latest one or two windows*, giving the sub-millisecond speeds, whereas older memory is accessed less frequently and can tolerate slightly higher latency if needed.

* **Temporal Decay of Recall:** By design, memory beyond the retention window is dropped (or optionally summarized). This is an intentional strategy: it favors *precision and speed for recent context over full recall of all past contexts*. Agents using MBFEC will typically combine this with strategies like *summary compaction* for older information (e.g. summarizing a day’s events into one node that is kept). The ring buffer ensures a hard cap on how far back raw memory can be recalled directly. This aligns with practical needs – an agent usually doesn’t need verbatim recall of something that happened months ago unless it was deemed important enough to summarize or keep.

* **Garbage Collection and Deletion:** Deletion in MBFEC must efficiently handle removal of batches of nodes (when a window expires) and occasionally individual nodes (e.g. if a memory needs to be retracted or redacted). The Bloom filter components are chosen to be **deletion-friendly**. Standard Bloom filters cannot remove individual entries, so MBFEC can employ alternatives:

  * *Counting Bloom Filters:* These use counters instead of single bits, allowing decrementation when removing an item. MBFEC can use counting filters for small windows to support occasional deletions at a bit-level cost.
  * *Cuckoo Filters or XOR Filters:* These are more modern probabilistic structures that can support deletion. For example, **XOR filters** offer similar false positive rates with less space and can be rebuilt quickly. While XOR filters themselves are static after construction, MBFEC leverages the window architecture to rebuild filters window-by-window when needed (since windows are of limited size, rebuilding is feasible upon major changes). In practice, when a window expires, its entire filter is simply dropped. If an individual node is deleted within an active window, a background task can either decrement its count in a counting BF or rebuild that window’s BF in an amortized way (especially if such deletions are rare).
  * The key is that each window’s index is independent, so deletion or addition operations are localized. This avoids having to update one huge global structure on each memory update. By partitioning time, MBFEC ensures **consistent performance** even as total memory grows – only the current window’s indices get frequent writes, older windows become read-only and then expire.

* **Freshness and Weighting:** This temporal structure also allows *temporal weighting*: the system can implicitly rank more recent results higher. Even if a slightly older memory has a similar embedding distance as a fresh memory, the framework might choose the fresh one first, under the assumption that recency correlates with relevance in a dynamic environment. (This can be tuned or left to the agent’s discretion, but the architecture makes it easy by searching newer windows first and only falling back if needed.)

In effect, MBFEC’s temporal indexing is like a **sliding context window** for the agent’s vector memory. It ensures the agent’s short-term memory is extremely **responsive and up-to-date**, while older memories gradually phase out or condense. This design is inspired by human-like memory which is strong in the short term and fuzzy in the long term, and by practical system constraints to keep retrieval fast.

### Efficiency and Implementation Considerations

To meet the performance goals, MBFEC employs several efficiency-focused strategies in its implementation:

* **CPU-Optimized Vector Operations:** All vector similarity computations and index traversals in the prototype are optimized for CPU (taking advantage of SIMD instructions like AVX2/AVX-512). This avoids reliance on GPUs for inference-time search. Using CPUs eliminates the overhead of transferring data to GPU for every query, which at low query concurrency can actually make GPU search slower. By keeping data in RAM and leveraging CPU caches, MBFEC can handle a high query rate with minimal latency. The design acknowledges that while GPUs can accelerate batch ANN searches, an interactive agent benefits from the *immediate availability* of data on CPU memory and the fine-grained parallelism of modern CPUs.

* **Memory Footprint and Compression:** As the memory grows (within the retained windows), MBFEC keeps the memory footprint efficient. Bloom filters are sized to maintain a low false positive rate (tunable, e.g. 1% or 0.1%) without using excessive RAM. Because Bloom filters are bit arrays, they are very space-efficient for representing membership of thousands of items. The HNSW index is the other main memory consumer; here we apply compression techniques (like storing vectors as int8 or using PQ coarse quantization) to reduce memory. Research and industry experience (e.g. Weaviate’s HNSW+PQ feature) show that compressing vectors can yield large memory savings with minimal impact on recall. MBFEC will use such methods so that even if an agent has, say, 100k recent memory nodes, the vector index remains lightweight.

* **Parallel and Lock-Free Operations:** The system will be designed for multi-threaded querying – the Bloom filter checks and graph search can be parallelized. Each stage of the cascade can run in its own thread or vectorized loop: e.g. check multiple Bloom filters concurrently (since they are independent per window or per layer), or evaluate distances to multiple HNSW candidates in parallel. Also, since Bloom filters are read-only during queries (writes happen on new memory insertion), we can make those checks lock-free and wait-free. Updates to indices (when adding a new memory node) will be batched or handled asynchronously if needed to not stall queries.

* **Tool Integration:** As an embedded prototype, MBFEC will be implemented in a systems programming language (like C++ or Rust) or leveraging high-performance libraries (e.g. `hnswlib` for Python prototypes). It will expose a clear API for the agent framework: e.g. `addMemory(node)` and `queryMemory(query_embedding, k)` which returns top-\$k\$ similar past nodes. This API abstracts away the cascade internals from the agent developer. In Phase 2, the implementation will evolve to a service model (accessible via network API or shared memory in a cluster), but with similar API semantics.

Overall, the architecture is designed for **efficiency at scale** – using *probabilistic filtering* and *hierarchical search* to achieve speed, and using *temporal segmentation* and *compression* to manage growth.

### Multi-Agent Memory Sharing and Graph Evolution (Phase 2)

In the cloud-based phase, MBFEC becomes a **shared memory platform** for multiple agents. The design supports scenarios where several agents (which could be separate services, or cooperating sub-agents) have access to a joint memory repository or overlapping memory graphs. Key aspects of the multi-agent extension:

* **Shared Memory Space:** Agents can contribute their individual memories into a *global memory graph* (or into group-specific memory pools). For example, if Agent A learns some fact during its task, Agent B could later retrieve that fact via MBFEC if they share the same memory space. This is implemented by allowing multiple writers to the memory index and tagging memory nodes with origin metadata (which agent, context, etc.). The retrieval API can then be made agent-aware (e.g. an agent might retrieve only its own memories plus a common pool, or truly everything depending on configuration).

* **Consistency and Synchronization:** In a distributed setting, memory updates need to be synchronized across nodes or processes. MBFEC employs a *hybrid synchronization protocol* inspired by CRDTs and distributed indices. Each agent or node may maintain a local replica of the memory index (or a partition of it). Periodically or upon certain triggers, agents exchange **summaries** of their memory (rather than full data). For instance, each memory segment can be assigned a **Merkle tree hash** that summarizes its contents. Agents exchange these hashes to detect differences in their memory states. If a difference is detected (meaning one agent has memories the other lacks), they then exchange more granular info for just those segments.

* **Probabilistic Filtering for Sync:** To minimize bandwidth in multi-agent sync, MBFEC uses Bloom filters in the exchange process as well. Each agent can send a **Bloom filter summary** of its recent memory (for a given window or timespan), so that the receiving side can quickly infer which specific memory IDs or vectors it might be missing. This avoids full scan comparisons – an agent doesn’t have to send all its memory data, only a cheap summary. Using Bloom filters in this way allows a form of *partial knowledge sharing*: an agent can essentially ask another “do you have any memory that looks like X?” by sending a query BF and getting a yes/no or a list of likely candidates.

* **Memory Compaction and Merging:** When multiple agents share memory, there may be redundant or highly similar entries (e.g. both agents read the same document or had the same conversation snippet). The system can perform **memory compaction** to merge these duplicative nodes. For example, two nearly identical memory nodes can be replaced by one canonical node with links pointing from both original contexts. Compaction can happen offline or during synchronization (e.g. if two peers realize they have overlapping content, they keep one copy and drop the other to save space). This requires semantic comparison – effectively using the same embedding similarity to detect overlaps – possibly with human or higher-level confirmation for safety. The result is a cleaner, more *consolidated knowledge base*.

* **Latent Link Prediction:** In a multi-agent memory graph, novel insights can emerge by connecting pieces from different agents. MBFEC will incorporate a module for **latent link prediction** that analyzes the combined memory graph to propose new edges. For example, Agent A’s node about “Topic X research results” could be linked to Agent B’s node about “Topic X client requirements” if the system predicts a meaningful connection. Technically, this can use graph embedding techniques or transformer models that look at pairs of memory entries and score their relatedness. Over time, this enriches the graph beyond what any single agent saw in isolation – effectively **cross-pollinating knowledge**. Such links might later trigger one agent to recall something another agent experienced, improving collective performance.

* **Access Control and Privacy:** (Though not explicitly requested, it’s a practical consideration.) In multi-agent mode, MBFEC will allow configuring which memories are shared vs private. Some memory may remain local to an agent (not inserted into the global index or only summarized at a high level). The system’s synchronization can operate on different channels, e.g. a shared channel for communal knowledge and a private channel for personal memory.

Functionally, multi-agent MBFEC enables things like a team of research assistants pooling their findings, or an AI assistant that retains organizational knowledge across different user sessions. It effectively creates a **distributed cognitive architecture**, where memory is not siloed per agent but rather an asset that can be tapped by any authorized agent.

A concrete scenario: consider two agents in an enterprise – one “Data Scientist Assistant” and one “DevOps Assistant” – both using MBFEC. The Data Science agent might discover an insight about a dataset and store it in memory. Later, the DevOps agent faces an issue that that insight could help with; the DevOps agent’s query retrieves the relevant memory (because the memory was shared), allowing it to solve the problem faster. MBFEC’s design ensures this happens seamlessly, with the same low-latency retrieval mechanisms, by treating all agents’ recent critical memories as part of one big temporal index (or at least overlapping indices). Synchronization and Bloom-filter summaries make sure each agent doesn’t miss out on new entries added by others.

## Detailed Specification and MVP Implementation Plan

This section outlines the functional and technical specifications in detail, and provides a modular plan for implementing a Minimum Viable Product (MVP) of MBFEC.

### Functional Specification

The MBFEC system shall provide the following functionality in the MVP (single-agent prototype):

* **Memory Insertion:** Ability to add a new memory node for each agent step, including storing the content and computing its embeddings. This will be exposed as an API (e.g. `addMemory(content, metadata)`), which returns a unique node ID.

* **Context Querying:** Ability to query the memory for relevant past nodes given the current context or query content. This is the core retrieval call (e.g. `queryMemory(query_content, top_k)` or similar). The system will return references to the most similar memory nodes (with similarity scores) within a short time bound (target <1ms for recent memory hits, and gracefully degrading to a few milliseconds if older windows are searched).

* **Realtime Updating:** The memory index updates in near-real-time with each insertion. Right after a new step is stored, it should be available for retrieval by subsequent queries (latency from insertion to availability ideally <10ms). The system must handle a stream of inserts (from the agent’s sequential steps) and interleave queries without significant performance loss.

* **Temporal Window Management:** The system automatically manages the windowing of memory. Users can configure parameters like window duration or maximum memory nodes to retain. Expired windows are removed without user intervention, effectively capping the memory size. The user/agent can optionally be notified or provided a hook when data is expired (in case summarization or offloading is desired at that point).

* **Memory Inspection (Tools for Debugging):** For development and debugging, the MVP will include tools or methods to inspect the memory state – e.g. how many nodes, how many windows, the contents of a Bloom filter (maybe in summary form), etc. This helps validate that the memory is storing and evicting as expected.

* **Thread Safety:** The MVP should allow concurrent reads and writes (since an agent might be inserting a new memory and querying the memory at almost the same time). The functional requirement is that queries either include the most recent insertion or not, but never return a corrupted intermediate state. In practice, we might use simple locks around updates or maintain two copies of indices during an update to ensure consistency.

* **Accuracy and Relevance:** Functionally, when the agent queries the memory, the returned results should be semantically relevant to the query. We expect that if the agent asks about a concept it has seen in the last N interactions, MBFEC will return that interaction (as long as it’s still in the retained window). In MVP testing, we will verify that obvious queries retrieve the expected memories (for example, querying with an embedding of a recently seen sentence returns that sentence’s node or related ones as top hit).

* **Use Case Coverage:** The MVP will focus on text-based memory (e.g. the content of each node is text or a textual summary). It will be demonstrated in a scenario such as a question-answering agent or a conversational agent that can refer back to earlier parts of the conversation. It should show the agent retrieving recent dialogue context beyond what the LLM context window holds, thus augmenting the agent’s effective memory.

(Functional extensions planned for Phase 2, like multi-agent features, are noted but not implemented in MVP. They include memory sharing, cross-agent querying, etc., and will be added in later iterations once the core is validated.)

### Technical Specification (MVP Architecture)

The MVP will implement the core single-machine architecture with the following modules:

1. **Embedding Module:** This component is responsible for generating the hierarchical embeddings for any given content. It will likely integrate a pre-trained model (e.g. a Sentence Transformer or OpenAI embedding model) to produce a high-dimensional vector for input text. We will then slice this vector into the predefined “matryoshka” sizes. Optionally, we might fine-tune or use a pre-trained *Matryoshka embedding model* (if available, such as the models by Kusupati et al. 2022) so that the truncated embeddings are optimized. For MVP, using a regular embedding and simply splitting it (which might degrade quality) is acceptable, but the design allows swapping in true Matryoshka-trained models for better performance. The output of this module for each node insertion is a structure like `{id, full_vec, vec_d1, vec_d2, ...}` where `vec_dN` is the embedding truncated to N% of full length as needed by the cascade.

2. **Memory Storage & Metadata:** A lightweight in-memory store (or database) holds the memory nodes. This can be as simple as a Python list or dictionary mapping IDs to node data for the prototype. It tracks the assignment of nodes to windows as well. For example, when inserting, it determines which window (perhaps based on timestamp or an index counter) the node belongs to, and appends it there. Each window can be an array of node references. The metadata includes timestamps to manage expiry.

3. **Bloom Filter Index Module:** This module manages the cascade of Bloom filters. Concretely, for each window and each embedding granularity, it maintains a Bloom filter structure. For MVP simplicity, we might start with only one or two layers (e.g. one Bloom filter on a coarse embedding hash and then directly HNSW on full embeddings) to validate the concept. As we progress, we implement multiple layers:

   * e.g. `BF_coarse_window_i`, `BF_medium_window_i` for each window i. The coarse BF might use a hash of the 1/4-length embedding as the key; the medium BF uses 1/2-length embedding.
   * We will use an existing Bloom filter library if available (ensuring it can handle our key sizes, which might mean hashing the vector down to a few bytes). Alternatively, implement a simple custom BF using Python `bitarray` or a C++ bitset for speed.
   * Insertion: When a new node is added, its truncated embeddings are hashed and the bits set in the respective Bloom filters of that window.
   * Query (for a single window): Hash the query’s truncated embedding and test membership in BF. If negative, we skip this window entirely at that granularity. If positive, we consider the window as a candidate for deeper search.
   * For MVP, we’ll likely tune Bloom filter size (m bits) and number of hash functions k based on expected window size to keep false positive rate reasonably low (\~1-5%).
   * We will **not** implement a learned BF in MVP (as that is more complex); instead we rely on static hashing. However, we design the BF module such that a learned model could be hooked in later (perhaps by providing a callback that filters candidates).

4. **HNSW Index Module:** We will integrate an HNSW library (e.g. `hnswlib` for Python, or Faiss with HNSW index in C++). The HNSW index can be maintained per window or globally. A straightforward approach for MVP: maintain a single HNSW index for all active memory (all windows combined). We can limit the number of items in it by removing expired nodes (most HNSW libraries don’t support deletion well, so an alternative is to rebuild or maintain one index per window).

   * Possibly, to avoid deletion issues, we will maintain a small HNSW index just for the *latest window*, which is updated frequently. For older windows, we keep separate indices or rely on BF only. However, since we do want to retrieve older info sometimes, we might maintain one HNSW index per window and search them sequentially (this is simpler logically).
   * **Compressed vectors:** If using `hnswlib`, it supports only float vectors by default. For MVP, we might skip compression and use full floats (given moderate data size). But we will keep the design open to adding a compression layer (e.g. storing PCA-reduced or quantized vectors).
   * We will configure HNSW parameters (M – max connections, ef – search budget) such that search is very fast and approximate. Typical small values (M=16, efSearch=10) might be enough for the small candidate sets after BF filtering.

5. **Retrieval Orchestrator:** This is the logic that ties everything together when a query is issued. Pseudocode for query might be:

   ```
   def queryMemory(query_content, k):
       q_full, q_d1, q_d2 = embed_module.generate(query_content)  # full and truncated embeddings
       results = []
       # check windows in descending order (newest first)
       for window in active_windows_sorted_newest:
           if BF_coarse_window passes(q_d2):    # e.g., coarse check
               if BF_medium_window passes(q_d1):  # e.g., finer check
                   # candidate: search HNSW of this window with q_full
                   candidates = hnsw_index_window.search(q_full, k)
                   results.extend(candidates)
           if len(results) >= k and some similarity threshold satisfied:
               break  # we found enough good results in recent windows
       return top_k(results)
   ```

   This orchestrator ensures we traverse the cascade as specified. For MVP, it may be acceptable to search each window’s HNSW and then combine results, since number of windows is small. If performance is an issue, we can introduce a global HNSW as an optimization later.

   * The orchestrator also handles cases like when no BF layers are used (it can directly fall back to full HNSW search, which is essentially a baseline). This helps in testing: we can compare cascade vs direct search to validate recall and speed improvements.

6. **Window Manager:** A small component (could just be part of the storage module) tracks the current window and triggers rotation. For MVP, we can simulate time windows by count: e.g. every N insertions, start a new window. The manager will then freeze the old window’s indices (mark them read-only) and initialize new Bloom filters and HNSW for the new window. If the number of active windows exceeds the retention limit, it will remove the oldest window’s data structures and free memory. Removal means also potentially removing those nodes from any global indices if present (for a per-window HNSW design, just drop the structure; for a single HNSW design, we may skip removal as MVP and rely on the fact that queries won’t consider IDs outside active range – a bit of technical debt to address later).

7. **Monitoring & Logging:** For development, incorporate logging to record query times, hits vs misses at each BF stage, etc. This will be invaluable for benchmarking and tuning (e.g. seeing if BF filters are too strict or too lenient). We might also log memory usage of each component periodically.

The MVP will be considered complete when we can demonstrate end-to-end that an agent’s recent dialogue can be stored and a query can retrieve the correct piece of information nearly instantaneously, using the cascade. We will likely implement the MVP in a high-level language first (Python) for ease, then optimize critical parts (e.g. BF and vector math) with C++/Cython or switch to a lower-level implementation once correctness is proven.

### Benchmarking and Validation Plan

To ensure MBFEC meets its design goals, we will conduct thorough benchmarking and validation:

* **Latency Benchmarks:** We will measure the end-to-end query latency under various scenarios. Key metrics:

  * *Recent context latency:* Time to retrieve a memory from the latest window. We expect this to be <1ms (when measured in a controlled environment on a single machine). We’ll test with increasing memory sizes in the latest window to see how latency scales.
  * *Older context latency:* Time to retrieve from the oldest retained window (when the query specifically matches something near the retention cutoff). This might be higher, but we aim to keep it low as well (perhaps a few milliseconds). We will verify that the system properly short-circuits once enough results are found from recent windows.
  * We will break down latency by stage: e.g. X microseconds in BF filters, Y in HNSW search. This helps identify bottlenecks. If BF checks are dominating, we might reduce their number or optimize hashing; if HNSW search is too slow, perhaps tune ef or use a smaller index.

* **Throughput and Scalability:** Although the primary aim is low latency for single queries, we will also measure how the system handles many queries in succession or slight concurrency. For example, can it sustain 1000 queries per second on one CPU core without lag? We will simulate an agent doing rapid-fire thinking (back-to-back queries) and see if any queuing occurs. Scalability tests will include growing the memory size (number of windows, number of nodes) to ensure the performance doesn’t degrade unexpectedly beyond design assumptions.

* **Recall and Accuracy Tests:** Since we use approximate methods (BFs and ANN search), we must verify that we aren’t missing relevant memories:

  * We will prepare a test dataset of queries and expected relevant memories. For instance, take a log of an agent’s conversation and at certain points insert questions that refer to earlier facts – then test if MBFEC indeed retrieves those facts.
  * We will compare MBFEC’s results with a brute-force embedding search (linear scan of all embeddings) for a sample of queries. This gives a ground truth for nearest neighbors. We can calculate recall\@k (the fraction of true nearest neighbors that MBFEC returned in its top-k). Our goal is to achieve very high recall for recent items (e.g. >0.9 recall\@5 for queries targeting last-window info), and acceptable recall for older ones given some tolerance.
  * We also measure false positives from BFs: how often does a BF indicate a window might have a result when it actually doesn’t? This is mostly a performance issue (wasted work) rather than an accuracy issue, but we want to tune the BF to minimize this overhead while keeping false negatives at zero.

* **Window Expiry Validation:** Test that the ring buffer mechanism works: when memory exceeds the limit, old nodes are actually not retrievable anymore and their resources are freed. For example, if we set retention to 3 windows and then add 4 windows worth of data, we confirm that queries for data from the first window fail (as expected) and that memory usage is roughly constant after many insertions (meaning old stuff is being dropped).

* **Deletion and Update Tests:** If the MVP supports deletion, we will test removing some nodes and checking that they indeed no longer appear in results (and ideally do not cause errors in indices). For updates (if any, though likely each node is immutable once added), we ensure new additions don’t corrupt existing retrievals.

* **Stress and Edge Cases:** We will test edge cases like querying with content that has no match in memory (system should return an empty result or a sensible fallback), or querying when memory is empty, etc. Also, extremely large pieces of text (to see if embedding generation or dimensionality reduction has any issues, though typically that’s outside MBFEC’s scope aside from slower embedding computation).

* **Benchmark Data:** We may use synthetic data to push the system – e.g. generate random embeddings and test the index throughput. But more meaningfully, we can use an open dataset like the **Conversational QA** logs or some **dialogue dataset** to simulate agent memory. Another idea is to use a benchmark like *SQuAD* or *TriviaQA* where a question should retrieve a supporting fact – insert the supporting facts as memory and see if MBFEC finds them given the question embedding.

* **Comparative Metrics:** If possible, compare MBFEC against a baseline vector store (like Faiss flat index or plain HNSW on full data without bloom filters). This will illustrate the efficiency gain. We expect MBFEC to outperform a flat index in query latency, especially as memory grows, because it limits how much needs to be scanned/searched for each query. Memory usage overhead (due to multiple embeddings and filters) will also be measured, to ensure it’s within acceptable limits (we trade some memory for speed, quantify how much).

All tests above will be automated in a test suite. We’ll particularly focus on **worst-case scenarios** for performance – e.g., query that isn’t present in recent window so it has to check all windows (ensuring even that is fast), or a pathological case where Bloom filters yield lots of false positives. This helps refine parameters and ensure robust performance.

For validation beyond metrics, we will gather qualitative feedback: e.g. have the agent run through a scenario using MBFEC memory and confirm it behaves more intelligently (remembers things correctly) compared to an agent without such memory. Success is measured by both the empirical numbers and the improved capabilities of the agent in realistic tasks.

## Draft Patent Claims (Innovation Highlights)

*(The following are draft patent claims for various novel aspects of the MBFEC system, grouped by category of innovation.)*

### **Efficiency and Performance**

1. **Ultra-Low Latency Memory Retrieval** – *Claim:* A method for retrieving context from an AI agent’s memory in sub-millisecond time, by hierarchically truncating an embedding of a query and progressively filtering candidate memory entries, such that recent memory entries are identified with minimal computation, thereby enabling real-time memory recall during agent inference.
2. **CPU-Optimized Vector Search without GPU Overhead** – *Claim:* A vector search architecture that achieves high throughput and low latency on standard CPUs by using SIMD-accelerated distance computations and avoiding data transfer to GPUs, wherein at low concurrency the search on CPU outperforms GPU-based search due to elimination of transfer overhead.
3. **Memory-Scalable Temporal Index** – *Claim:* A memory indexing scheme that maintains bounded index size over time by expiring old segments in a ring buffer fashion, ensuring constant-time query performance even as total stored events grow, thus achieving a predictable memory footprint and query speed regardless of the age of the memory.

### **Retrieval Architecture**

4. **Matryoshka Embedding Cascade Retrieval** – *Claim:* A multi-stage retrieval process using nested embeddings of decreasing dimensionality (Matryoshka embeddings) to first execute a coarse similarity search and then iteratively refine results with higher-dimensional representations, performing only a single initial broad search followed by in-memory re-ranking at successive detail levels. This cascade significantly reduces the search space while preserving recall of relevant items.
5. **Bloom-Filtered Vector Search Pipeline** – *Claim:* An integrated pipeline for approximate nearest neighbor search that interposes Bloom filter checks at one or more stages of vector processing, wherein each Bloom filter corresponds to a quantized or hashed representation of vector data at a certain granularity, and filters out a large fraction of unrelated vectors before a finer similarity search is performed. The result is a dramatic reduction in candidate comparisons needed for identifying nearest neighbors.
6. **Hybrid ANN Graph Re-Ranking** – *Claim:* The combination of a probabilistic filtering stage with a graph-based ANN index (HNSW) as a re-ranking mechanism, wherein the filtering stage yields a candidate set and the graph index then navigates those candidates’ neighborhood structure to precisely rank the top results. This hybrid approach retains the accuracy of graph-based search with fewer distance evaluations, optimizing both speed and precision.

### **Learned Filtering and Adaptive Indexing**

7. **Learned Bloom Filter Cascade** – *Claim:* The use of a sequence of learned models coupled with Bloom filters to progressively narrow down relevant memory nodes, wherein each learned model predicts membership of a query in the set of vectors within a certain distance threshold, and each Bloom filter is tuned based on the model’s characteristics to achieve an optimal balance of false positives and computation. Through dynamic programming or training, the system optimally allocates bits between models and filters to minimize overall rejection time.
8. **Distance-Sensitive Hashing Filters** – *Claim:* A mechanism for embedding vectors into hash codes such that the probability of a hash collision is a controlled function of the distance between the original vectors, and inserting these hashes into a probabilistic filter (Bloom filter or XOR filter). The filter thus serves as a quick distance-sensitive check: any query vector that does not produce a hash collision with stored items is immediately rejected as dissimilar, accelerating the search for near neighbors.
9. **Multi-Granularity Indexing** – *Claim:* An index structure that stores multiple representations of each item (for example, an item’s embedding at 25%, 50%, 100% of full dimensionality) and uses the coarser representations to index or partition the search space. Queries are run against progressively finer representations only for those partitions or items that passed the coarse tests. This multi-granular approach provides adjustable trade-offs between search recall and speed, allowing the system to adapt to different performance requirements on the fly.

### **Temporal Indexing and Memory Management**

10. **Sliding Window Vector Memory** – *Claim:* A temporal memory system for AI agents that divides the continuous stream of experiences into discrete time windows and indexes each window separately, automatically discarding or compressing windows older than a configured horizon. The system ensures that memory lookup cost remains primarily a function of the window size (recent memory) and not the total accumulated history, enabling consistent real-time recall of recent events.
11. **Deletion-Friendly Probabilistic Index** – *Claim:* A deployment of a deletion-capable probabilistic data structure (such as a counting Bloom filter, cuckoo filter, or XOR filter) within a memory index, allowing individual memory entries or entire segments to be removed without needing a full index rebuild. This claim covers the integration of such filters in a time-segmented memory where expired segments can be dropped and their filter states cleared or recycled, providing efficient memory turnover in long-running systems.
12. **Temporal Relevance Weighting** – *Claim:* A method of implicitly boosting the relevance of recent memories by structuring the search to prioritize newer memory segments. The query mechanism searches newer segments first and only falls back to older segments if necessary, thereby weighting the agent’s decision-making in favor of recent context without requiring explicit numeric decay factors. This architecture-level bias towards recency improves the practical relevance of retrieved information in temporal workflows.

### **Multi-Agent Memory Sharing and Knowledge Integration**

13. **Shared Memory Graph for Multiple Agents** – *Claim:* A memory architecture in which multiple AI agents contribute to and retrieve from a shared vector-based memory graph. Each memory node in the graph can have multiple “owners” or references from different agents, and queries from any agent can surface nodes created by others, subject to access policies. This fosters a collective memory that grows with each agent’s experiences and benefits all participating agents.

14. **Memory Synchronization via Bloom Filters** – *Claim:* A synchronization protocol for distributed memory indexes where agents exchange Bloom filters (or similar probabilistic summaries) of their memory contents to identify which memory nodes are missing from each peer. Upon detecting a difference (e.g., via comparing hashes of memory states and then Bloom filters), the protocol selectively transmits only the missing items or summaries, significantly reducing communication overhead during multi-agent memory merge or update.

15. **Latent Link Formation in Memory Graphs** – *Claim:* An automated process for enhancing a memory graph by adding new edges between memory nodes based on a learned similarity or predictive model (latent link prediction). The system periodically or continuously evaluates pairs or clusters of nodes (potentially contributed by different agents) to detect implicit relationships – such as common themes or causal links – and updates the graph with these relationships. This leads to emergent knowledge not explicitly encoded by any single agent, improving future retrieval by connecting conceptually related experiences across the memory graph.

16. **Memory Compaction through Node Merging** – *Claim:* A method for managing long-term memory growth by identifying semantically overlapping or redundant memory nodes and merging them into a single representative node. The merge can involve combining content or selecting one node as primary and linking others to it, along with updating the index (embeddings and filters) to reflect the merged representation. In a multi-agent context, this includes merging memories of similar content from different agents to eliminate duplication and reduce memory usage while preserving the information (e.g., merging two agents’ identical observations into one memory entry referenced by both).

These claims highlight the innovative techniques MBFEC introduces in efficient semantic memory retrieval, adaptive indexing, temporal management, and collaborative agent memory – collectively pushing the state-of-the-art for AI agent long-term memory systems. The combination of hierarchical embeddings with probabilistic filtering and the extension to multi-agent shared memory is unique to the MBFEC approach and provides both practical performance benefits and novel capabilities for future intelligent systems.
